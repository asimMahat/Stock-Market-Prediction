{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3799f9-2331-4200-ac05-ad0c739a152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:05:32.567582: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb88a9b0-6c6b-4b4d-8cc3-3cb8632f4fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962-01-02 00:00:00-05:00</td>\n",
       "      <td>1.518550</td>\n",
       "      <td>1.518550</td>\n",
       "      <td>1.501487</td>\n",
       "      <td>407940</td>\n",
       "      <td>1.501487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1962-01-03 00:00:00-05:00</td>\n",
       "      <td>1.501487</td>\n",
       "      <td>1.514612</td>\n",
       "      <td>1.501487</td>\n",
       "      <td>305955</td>\n",
       "      <td>1.514612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962-01-04 00:00:00-05:00</td>\n",
       "      <td>1.514613</td>\n",
       "      <td>1.514613</td>\n",
       "      <td>1.498863</td>\n",
       "      <td>274575</td>\n",
       "      <td>1.499519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1962-01-05 00:00:00-05:00</td>\n",
       "      <td>1.497551</td>\n",
       "      <td>1.497551</td>\n",
       "      <td>1.467363</td>\n",
       "      <td>384405</td>\n",
       "      <td>1.469988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1962-01-08 00:00:00-05:00</td>\n",
       "      <td>1.468675</td>\n",
       "      <td>1.468675</td>\n",
       "      <td>1.430613</td>\n",
       "      <td>572685</td>\n",
       "      <td>1.442425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date      Open      High       Low  Volume     Close\n",
       "0  1962-01-02 00:00:00-05:00  1.518550  1.518550  1.501487  407940  1.501487\n",
       "1  1962-01-03 00:00:00-05:00  1.501487  1.514612  1.501487  305955  1.514612\n",
       "2  1962-01-04 00:00:00-05:00  1.514613  1.514613  1.498863  274575  1.499519\n",
       "3  1962-01-05 00:00:00-05:00  1.497551  1.497551  1.467363  384405  1.469988\n",
       "4  1962-01-08 00:00:00-05:00  1.468675  1.468675  1.430613  572685  1.442425"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"ibm.csv\"\n",
    "file_cleaned_path = \"cleaned_ibm.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data_filtered = data[[\"Date\", \"Open\", \"High\", \"Low\", \"Volume\", \"Close\"]].sort_values(by=\"Date\")\n",
    "data_cleaned = data_filtered.dropna()\n",
    "\n",
    "data_cleaned.to_csv(file_cleaned_path, index=False)\n",
    "data = pd.read_csv(file_cleaned_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8e614b-40e6-4b80-9d0e-41d94712cea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962-01-02 00:00:00-05:00</td>\n",
       "      <td>1.518550</td>\n",
       "      <td>1.518550</td>\n",
       "      <td>1.501487</td>\n",
       "      <td>407940</td>\n",
       "      <td>1.501487</td>\n",
       "      <td>1.514612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1962-01-03 00:00:00-05:00</td>\n",
       "      <td>1.501487</td>\n",
       "      <td>1.514612</td>\n",
       "      <td>1.501487</td>\n",
       "      <td>305955</td>\n",
       "      <td>1.514612</td>\n",
       "      <td>1.499519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962-01-04 00:00:00-05:00</td>\n",
       "      <td>1.514613</td>\n",
       "      <td>1.514613</td>\n",
       "      <td>1.498863</td>\n",
       "      <td>274575</td>\n",
       "      <td>1.499519</td>\n",
       "      <td>1.469988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1962-01-05 00:00:00-05:00</td>\n",
       "      <td>1.497551</td>\n",
       "      <td>1.497551</td>\n",
       "      <td>1.467363</td>\n",
       "      <td>384405</td>\n",
       "      <td>1.469988</td>\n",
       "      <td>1.442425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1962-01-08 00:00:00-05:00</td>\n",
       "      <td>1.468675</td>\n",
       "      <td>1.468675</td>\n",
       "      <td>1.430613</td>\n",
       "      <td>572685</td>\n",
       "      <td>1.442425</td>\n",
       "      <td>1.459488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date      Open      High       Low  Volume     Close  \\\n",
       "0  1962-01-02 00:00:00-05:00  1.518550  1.518550  1.501487  407940  1.501487   \n",
       "1  1962-01-03 00:00:00-05:00  1.501487  1.514612  1.501487  305955  1.514612   \n",
       "2  1962-01-04 00:00:00-05:00  1.514613  1.514613  1.498863  274575  1.499519   \n",
       "3  1962-01-05 00:00:00-05:00  1.497551  1.497551  1.467363  384405  1.469988   \n",
       "4  1962-01-08 00:00:00-05:00  1.468675  1.468675  1.430613  572685  1.442425   \n",
       "\n",
       "     Target  \n",
       "0  1.514612  \n",
       "1  1.499519  \n",
       "2  1.469988  \n",
       "3  1.442425  \n",
       "4  1.459488  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Target\"] = data[\"Close\"].shift(-1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4e87e1-65cb-4413-88aa-b3dd21f87c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15825</th>\n",
       "      <td>2024-11-13 00:00:00-05:00</td>\n",
       "      <td>209.5</td>\n",
       "      <td>211.410004</td>\n",
       "      <td>209.070099</td>\n",
       "      <td>2186158</td>\n",
       "      <td>210.669998</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Date   Open        High         Low   Volume  \\\n",
       "15825  2024-11-13 00:00:00-05:00  209.5  211.410004  209.070099  2186158   \n",
       "\n",
       "            Close  Target  \n",
       "15825  210.669998     NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_row = data.tail(1)\n",
    "data.drop(data.tail(1).index, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "final_data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965163d1-961e-4d2e-a579-cc0e9664b551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open      0.999714\n",
       "High      0.999765\n",
       "Low       0.999767\n",
       "Volume    0.133558\n",
       "Close     0.999812\n",
       "Target    1.000000\n",
       "Name: Target, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, 1:].corr()['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d55637cb-c1ae-41ad-bfb8-c8ffc543a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features scaling\n",
    "model_features = data.drop(\"Target\", axis=1).drop(\"Date\", axis=1)\n",
    "model_target = data[\"Target\"]\n",
    "\n",
    "model_feature_scaler = MinMaxScaler()\n",
    "model_feature_scaler.fit(model_features)\n",
    "model_scaled_features = pd.DataFrame(model_feature_scaler.transform(model_features), columns=model_features.columns.tolist())\n",
    "\n",
    "model_target_scaler = MinMaxScaler()\n",
    "model_target_scaler.fit(model_target.values.reshape(-1,1))\n",
    "model_scaled_target = pd.DataFrame(model_target_scaler.transform(model_target.values.reshape(-1,1)), columns=[\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1461627e-beb4-4777-9943-e448300405f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X_data, y_data, t_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(X_data) - t_steps):\n",
    "        v = X_data.iloc[i:(i + t_steps)].values\n",
    "        X.append(v)\n",
    "        y.append(y_data.iloc[i + t_steps])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3151a7d-8aab-4554-abfa-7c46e3f28fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15765, 60, 5), (15765, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_steps = 60\n",
    "X, y = create_dataset(model_scaled_features, model_scaled_target, t_steps)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ba830c-6687-42bf-b780-aeb5f8afa1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12612, 60, 5), (3153, 60, 5), (12612, 1), (3153, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.80 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2db885c-4fe9-4e32-b7f9-7734f4ce87a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:05:54.602257: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2024-11-30 14:05:54.607783: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2027] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 8.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "2024-11-30 14:05:55.678641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 541 MB memory:  -> device: 0, name: NVIDIA A30, pci bus id: 0000:17:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([12612, 60, 5]),\n",
       " TensorShape([12612, 1]),\n",
       " TensorShape([3153, 60, 5]),\n",
       " TensorShape([3153, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c7cbf7-4887-4b50-a024-d569082eafdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (time_step, features)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9afbeab-1920-4071-a44a-741239bfd7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU model\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "gru_model = Sequential([\n",
    "    tf.keras.layers.GRU(units=50, return_sequences=True, input_shape=input_shape),\n",
    "    tf.keras.layers.GRU(units=50),\n",
    "    Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe45c21a-060f-4c68-bbb7-37d3e594a350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 60, 50)            8550      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 50)                15300     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,901\n",
      "Trainable params: 23,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a7d0381-e4e3-4965-9b65-c298e836a4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:06:14.454067: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2024-11-30 14:06:14.454188: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 550.54.15\n",
      "2024-11-30 14:06:14.454212: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at cudnn_rnn_ops.cc:1554 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/gru/PartitionedCall]] [Op:__inference_train_function_5784]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m gru_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Training the GRU model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m history_gru \u001b[38;5;241m=\u001b[39m \u001b[43mgru_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/software/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/software/software/TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/gru/PartitionedCall]] [Op:__inference_train_function_5784]"
     ]
    }
   ],
   "source": [
    "# Compiling the GRU model\n",
    "gru_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Training the GRU model\n",
    "history_gru = gru_model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf058c-5a78-4fa7-8089-dba3f1195650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and evaluate GRU model\n",
    "gru_predictions = gru_model.predict(X_test)\n",
    "\n",
    "# Add dummy columns to match the original scaled data shape (5 columns)\n",
    "dummy_columns = np.zeros((gru_predictions.shape[0], 4))  # 4 dummy columns to match Open, High, Low, Volume\n",
    "predicted_prices_scaled = np.concatenate((dummy_columns, gru_predictions), axis=1)\n",
    "\n",
    "# Apply inverse transformation to get back to original scale\n",
    "predicted_prices = model_feature_scaler.inverse_transform(predicted_prices_scaled)[:, -1]  # Get only the Close column\n",
    "\n",
    "# Similarly for actual prices\n",
    "actual_prices_scaled = np.concatenate((dummy_columns, y_test.numpy().reshape(-1, 1)), axis=1)\n",
    "actual_prices = model_feature_scaler.inverse_transform(actual_prices_scaled)[:, -1]\n",
    "\n",
    "gru_mse, gru_rmse, gru_mae = calculate_metrics(actual_prices, predicted_prices)\n",
    "print(f'GRU Model - MSE: {gru_mse:.4f}, RMSE: {gru_rmse:.4f}, MAE: {gru_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38899d-675b-4720-b20d-2a9deb77077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_gru.history['loss'], label='Training Loss')\n",
    "plt.plot(history_gru.history['val_loss'], label='Validation Loss')\n",
    "plt.title(' GRU Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ddbc9d-bebc-4a1a-8da1-9de18f2aff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting code with log scale\n",
    "plt.plot(history_gru.history['loss'], label='Training Loss')\n",
    "plt.plot(history_gru.history['val_loss'], label='Validation Loss')\n",
    "plt.title('GRU Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.yscale('log')  # Setting the y-axis to logarithmic scale\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ece95-089c-47ef-a98d-72e6d2fb798c",
   "metadata": {},
   "source": [
    "### Predicted vs actual price curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cbc89b-78c2-4db1-a8a0-2b709010c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(actual_prices, color=\"blue\", label=\"Actual IBM Stock Price\")\n",
    "plt.plot(predicted_prices, color=\"red\", label=\"Predicted IBM Stock Price by GRU\")\n",
    "plt.title(\"IBM Stock Price Predictions - GRU \")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75899dfc-1b42-4b68-9970-23b2638e54a3",
   "metadata": {},
   "source": [
    "### Random search for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a556cf1-87c8-4884-a9ff-b478e21bb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "param_ranges = {\n",
    "    \"units\": (32, 256), \n",
    "    \"batch_size\": (16, 128),\n",
    "    \"learning_rate\": (1e-5, 1e-2),\n",
    "    \"time_steps\": (30, 120) }\n",
    "\n",
    "n_samples = 20\n",
    "\n",
    "def generate_random_params():\n",
    "    units = random.randint(param_ranges[\"units\"][0], param_ranges[\"units\"][1])\n",
    "    batch_size = random.randint(param_ranges[\"batch_size\"][0], param_ranges[\"batch_size\"][1])\n",
    "    learning_rate = random.uniform(param_ranges[\"learning_rate\"][0], param_ranges[\"learning_rate\"][1])\n",
    "    time_steps = random.randint(param_ranges[\"time_steps\"][0], param_ranges[\"time_steps\"][1])\n",
    "    return units, batch_size, learning_rate, time_steps\n",
    "\n",
    "def evaluate_model(units, batch_size, learning_rate, time_steps):\n",
    "\n",
    "    global X_train, y_train, X_test, y_test\n",
    "    X, y = create_dataset(model_scaled_features, model_scaled_target, time_steps)\n",
    "    train_size = int(0.80 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    \n",
    "    input_shape = (time_steps, X_train.shape[2])\n",
    "\n",
    "    gru_model = Sequential([\n",
    "        tf.keras.layers.GRU(units=units, return_sequences=True, input_shape=input_shape,\n",
    "        tf.keras.layers.GRU(units=units),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    gru_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0)\n",
    "\n",
    "\n",
    "    # Training model\n",
    "    history = gru_model.fit(\n",
    "        X_train_tensor, y_train_tensor,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test_tensor, y_test_tensor),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    return val_loss\n",
    "\n",
    "# Performing random search\n",
    "\n",
    "# Number of random samples \n",
    "n_samples= 20 \n",
    "best_params = None\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for i in range(n_samples):\n",
    "    units, batch_size, learning_rate, time_steps = generate_random_params()\n",
    "\n",
    "    # Log the parameters being tested\n",
    "    print(f\"\\nTesting Hyperparameters: Units = {units}, Batch Size = {batch_size}, \"\n",
    "          f\"Learning Rate = {learning_rate:.6f}, Time Steps = {time_steps}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss = evaluate_model(units, batch_size, learning_rate, time_steps)\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "    # Log the result\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # Update best parameters if the current loss is lower\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_params = (units, batch_size, learning_rate, time_steps)\n",
    "\n",
    "# Print the best hyperparameters and their validation loss\n",
    "print(f\"Best Parameters: Units = {best_params[0]} Batch Size = {best_params[1]}, \"\n",
    "      f\"Learning Rate = {best_params[2]:.6f}, Time Steps (t_steps) = {best_params[3]}\")\n",
    "print(f\"Best Validation Loss: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b1e4f-5f8f-4149-9971-5adbf0935866",
   "metadata": {},
   "source": [
    "## Now using these parameters to test GRU model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bfafe-2a5b-4709-8f24-581314fba35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "t_steps = 107\n",
    "X, y = create_dataset(model_scaled_features, model_scaled_target, t_steps)\n",
    "\n",
    "train_size = int(0.80 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "#converting into tensor \n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "#batch size and no of epochs\n",
    "num_epochs = 30\n",
    "batch_size = 24\n",
    "\n",
    "model_optimized = Sequential([\n",
    "        tf.keras.layers.GRU(units=units, return_sequences=True, input_shape=input_shape,\n",
    "        tf.keras.layers.GRU(units=units),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "model_optimized.compile(optimizer=Adam(learning_rate=0.003813), loss='mean_squared_error')\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     patience=10,\n",
    "#     restore_best_weights=True\n",
    "# )\n",
    "\n",
    "# # Reducing learning rate when plateau is reached\n",
    "# reduce_lr = ReduceLROnPlateau(\n",
    "#     monitor='val_loss',\n",
    "#     factor=0.2,\n",
    "#     patience=5,\n",
    "#     min_lr=1e-5\n",
    "# )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0)\n",
    "\n",
    "\n",
    "history_gru = model_optimized.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df7eed-8b27-425e-b852-343edf6a9c5d",
   "metadata": {},
   "source": [
    "## Again plotting the graphs based on new set of optimized hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aea45c-ffed-4387-8430-2b8a93c28372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return mse, rmse, mae\n",
    "\n",
    "predictions = model_optimized.predict(X_test)\n",
    "\n",
    "# Adding dummy columns to match the original scaled data shape (5 columns)\n",
    "# 4 dummy columns to match Open, High, Low, Volume\n",
    "dummy_columns = np.zeros((predictions.shape[0], 4))\n",
    "predicted_prices_scaled = np.concatenate((dummy_columns, predictions), axis=1)\n",
    "\n",
    "# Applying inverse transformation to get back to original scale\n",
    "predicted_prices = model_feature_scaler.inverse_transform(predicted_prices_scaled)[:, -1]\n",
    "\n",
    "# for actual prices\n",
    "actual_prices_scaled = np.concatenate((dummy_columns, y_test.numpy().reshape(-1, 1)), axis=1)\n",
    "actual_prices = model_feature_scaler.inverse_transform(actual_prices_scaled)[:, -1]\n",
    "\n",
    "# Calculating metrics\n",
    "lstm_mse, lstm_rmse, lstm_mae = calculate_metrics(actual_prices, predicted_prices)\n",
    "print(f'GRU Model - MSE: {lstm_mse:.4f}, RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83768cc-5a60-44dc-b51d-ced21958d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(history_rnn.history['loss'], label='Training Loss')\n",
    "plt.plot(history_rnn.history['val_loss'], label='Validation Loss')\n",
    "plt.title(' RNN Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc024831-4aba-471b-ae59-72edafebfa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Your existing plotting code with log scale\n",
    "plt.plot(history_gru.history['loss'], label='Training Loss')\n",
    "plt.plot(history_gru.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.yscale('log')  # Setting the y-axis to logarithmic scale\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e196718-a761-480a-bbb7-5434fd45936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted and actual price plot \n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(actual_prices, color=\"blue\", label=\"Actual IBM Stock Price\")\n",
    "plt.plot(predicted_prices, color=\"red\", label=\"Predicted IBM Stock Price by RNN\")\n",
    "plt.title(\"IBM Stock Price Predictions - RNN \")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501938a-effd-4bec-9802-fb0966e17123",
   "metadata": {},
   "source": [
    "### Using PSO for optimizing hyperparameter for GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568cc6d-f561-4c2f-a7fc-02fbfdae6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pyswarm\n",
    "from pyswarm import pso\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "param_ranges = {\n",
    "    \"units\": (32, 256),       # Range for LSTM units\n",
    "    \"batch_size\": (16, 128),       # Range for batch size\n",
    "    \"learning_rate\": (1e-5, 1e-2), # Range for learning rate\n",
    "    \"t_steps\": (30, 120)           # Range for sequence length\n",
    "}\n",
    "\n",
    "bounds = [\n",
    "    param_ranges[\"units\"],       # Bounds for LSTM units\n",
    "    param_ranges[\"batch_size\"],       # Bounds for batch size\n",
    "    param_ranges[\"learning_rate\"],    # Bounds for learning rate\n",
    "    param_ranges[\"t_steps\"]           # Bounds for sequence length\n",
    "]\n",
    "\n",
    "def objective_function(params):\n",
    "    units = int(params[0])  \n",
    "    batch_size = int(params[1])\n",
    "    learning_rate = float(params[2])\n",
    "    t_steps = int(params[3])\n",
    "\n",
    "    print(f\"\\nTrying Hyperparameters: Units = {units}, Batch Size = {batch_size}, \"\n",
    "          f\"Learning Rate = {learning_rate:.6f}, Sequence Length (t_steps) = {t_steps}\")\n",
    "\n",
    "    # Updating sequence length for data creation\n",
    "    global X_train, y_train, X_test, y_test\n",
    "    X, y = create_dataset(model_scaled_features, model_scaled_target, t_steps)\n",
    "    train_size = int(0.80 * len(X))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    # Converting to tensors\n",
    "    X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "    y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    \n",
    "    input_shape = (t_steps, X_train.shape[2])\n",
    "\n",
    "    # Defining the model architecture\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.GRU(units=units, return_sequences=True, input_shape=input_shape,\n",
    "        tf.keras.layers.GRU(units=units),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0)\n",
    "\n",
    "    # Training the model\n",
    "    history = model.fit(\n",
    "        X_train_tensor, y_train_tensor,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test_tensor, y_test_tensor),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # validation loss\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Running PSO \n",
    "best_params, best_loss = pso(\n",
    "    objective_function,\n",
    "    lb=[b[0] for b in bounds],  # Lower bounds\n",
    "    ub=[b[1] for b in bounds],  # Upper bounds\n",
    "    swarmsize=5,               # Number of particles\n",
    "    maxiter=5,                 # Number of iterations\n",
    "    debug=True                  # Enable logging\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Parameters: Units = {int(best_params[0])}, Batch Size = {int(best_params[1])}, \"\n",
    "      f\"Learning Rate = {best_params[2]:.6f}, Sequence Length (t_steps) = {int(best_params[3])}\")\n",
    "print(f\"Best Validation Loss: {best_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c71b1d-38dd-41cd-8241-8306984349f0",
   "metadata": {},
   "source": [
    "### Now using the hyperparameters from PSO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d0078-91a1-49c1-be8c-e28d7fef32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "t_steps = 107\n",
    "X, y = create_dataset(model_scaled_features, model_scaled_target, t_steps)\n",
    "\n",
    "train_size = int(0.80 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "#converting into tensor \n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "#batch size and no of epochs\n",
    "num_epochs = 30\n",
    "batch_size = 24\n",
    "\n",
    "model_optimized = Sequential([\n",
    "        tf.keras.layers.GRU(units=units, return_sequences=True, input_shape=input_shape,\n",
    "        tf.keras.layers.GRU(units=units),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "model_optimized.compile(optimizer=Adam(learning_rate=0.003813), loss='mean_squared_error')\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     patience=10,\n",
    "#     restore_best_weights=True\n",
    "# )\n",
    "\n",
    "# # Reducing learning rate when plateau is reached\n",
    "# reduce_lr = ReduceLROnPlateau(\n",
    "#     monitor='val_loss',\n",
    "#     factor=0.2,\n",
    "#     patience=5,\n",
    "#     min_lr=1e-5\n",
    "# )\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0)\n",
    "\n",
    "\n",
    "history_gru = model_optimized.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd3b7b-8d14-4aa6-85d1-dc96c1fb079a",
   "metadata": {},
   "source": [
    "## Again plotting the graphs based on new set of optimized hyperparameters from PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea340a-01fc-4ace-b112-ce7c142feee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return mse, rmse, mae\n",
    "\n",
    "predictions = model_optimized.predict(X_test)\n",
    "\n",
    "# Adding dummy columns to match the original scaled data shape (5 columns)\n",
    "# 4 dummy columns to match Open, High, Low, Volume\n",
    "dummy_columns = np.zeros((predictions.shape[0], 4))\n",
    "predicted_prices_scaled = np.concatenate((dummy_columns, predictions), axis=1)\n",
    "\n",
    "# Applying inverse transformation to get back to original scale\n",
    "predicted_prices = model_feature_scaler.inverse_transform(predicted_prices_scaled)[:, -1]\n",
    "\n",
    "# for actual prices\n",
    "actual_prices_scaled = np.concatenate((dummy_columns, y_test.numpy().reshape(-1, 1)), axis=1)\n",
    "actual_prices = model_feature_scaler.inverse_transform(actual_prices_scaled)[:, -1]\n",
    "\n",
    "# Calculating metrics\n",
    "lstm_mse, lstm_rmse, lstm_mae = calculate_metrics(actual_prices, predicted_prices)\n",
    "print(f'GRU Model - MSE: {lstm_mse:.4f}, RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d3d5d-f944-4190-9baa-8d395846659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training vs validation loss plot\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(history_gru.history['loss'], label='Training Loss')\n",
    "plt.plot(history_gru.history['val_loss'], label='Validation Loss')\n",
    "plt.title(' RNN Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153b9d8-9020-4a59-a527-d013c266f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training vs validation loss plot based on log scale \n",
    "\n",
    "%matplotlib inline\n",
    "# Your existing plotting code with log scale\n",
    "plt.plot(history_gru.history['loss'], label='Training Loss')\n",
    "plt.plot(history_gru.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.yscale('log')  # Setting the y-axis to logarithmic scale\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c25d3-0acc-43fe-875a-168c67ed6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted and actual price plot \n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(actual_prices, color=\"blue\", label=\"Actual IBM Stock Price\")\n",
    "plt.plot(predicted_prices, color=\"red\", label=\"Predicted IBM Stock Price by RNN\")\n",
    "plt.title(\"IBM Stock Price Predictions - RNN \")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953b72a-d0a1-4d65-831f-83489c70f965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4-TensorFlow-2.11.0-cuda [jupyter_python]",
   "language": "python",
   "name": "sys_python_3.10.4-tensorflow-2.11.0-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
